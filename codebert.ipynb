{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10302386,"sourceType":"datasetVersion","datasetId":6376927}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport pandas as pd\nimport numpy as np\nfrom transformers import AutoTokenizer, AutoModel\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nimport logging\n\n# Reduce logging verbosity\nlogging.basicConfig(level=logging.WARNING)\n\nclass CodeBertTransformer(nn.Module):\n    def __init__(\n        self,\n        service_num_labels,\n        activity_num_labels,\n        model_name=\"microsoft/codebert-base\"\n    ):\n        super().__init__()\n\n        # Load CodeBERT model\n        self.transformer = AutoModel.from_pretrained(model_name)\n\n        # Freeze initial layers\n        for param in list(self.transformer.parameters())[:6]:\n            param.requires_grad = False\n\n        # Dropout for regularization\n        self.dropout = nn.Dropout(0.3)\n\n        # Advanced classification heads\n        self.service_classifier = nn.Sequential(\n            nn.Linear(self.transformer.config.hidden_size, 512),\n            nn.BatchNorm1d(512),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(512, service_num_labels)\n        )\n\n        self.activity_classifier = nn.Sequential(\n            nn.Linear(self.transformer.config.hidden_size, 512),\n            nn.BatchNorm1d(512),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(512, activity_num_labels)\n        )\n\n    def forward(self, input_ids, attention_mask):\n        # Efficient forward pass\n        outputs = self.transformer(\n            input_ids=input_ids,\n            attention_mask=attention_mask\n        )\n\n        # Use CLS token representation\n        pooled_output = outputs.last_hidden_state[:, 0, :]\n        pooled_output = self.dropout(pooled_output)\n\n        # Classification\n        service_pred = self.service_classifier(pooled_output)\n        activity_pred = self.activity_classifier(pooled_output)\n\n        return service_pred, activity_pred\n\nclass CodeBertDataset(Dataset):\n    def __init__(\n        self,\n        texts,\n        service_labels,\n        activity_labels,\n        tokenizer,\n        max_length=128\n    ):\n        # Efficient tokenization\n        self.encodings = tokenizer(\n            texts.tolist(),\n            truncation=True,\n            padding=True,\n            max_length=max_length,\n            return_tensors='pt'\n        )\n\n        self.service_labels = torch.tensor(service_labels, dtype=torch.long)\n        self.activity_labels = torch.tensor(activity_labels, dtype=torch.long)\n\n    def __len__(self):\n        return len(self.service_labels)\n\n    def __getitem__(self, idx):\n        return {\n            'input_ids': self.encodings['input_ids'][idx],\n            'attention_mask': self.encodings['attention_mask'][idx],\n            'service_label': self.service_labels[idx],\n            'activity_label': self.activity_labels[idx]\n        }\n\nclass CodeBertClassifier:\n    def __init__(\n        self,\n        training_data_path,\n        model_name=\"microsoft/codebert-base\"\n    ):\n        # Efficient device selection\n        self.device = torch.device(\n            'cuda' if torch.cuda.is_available() else 'cpu'\n        )\n        print(f\"Using device: {self.device}\")\n\n        # Load data efficiently\n        self.training_df = pd.read_csv(\n            training_data_path,\n            low_memory=False\n        )\n\n        # Prepare data\n        self._prepare_data(model_name)\n\n    def _prepare_data(self, model_name):\n        # Validate and clean data\n        self.training_df['service'] = self.training_df['service'].fillna('Unknown')\n        self.training_df['activityType'] = self.training_df['activityType'].fillna('Unknown')\n\n        # Tokenizer\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n\n        # Label encoding\n        self.service_encoder = LabelEncoder()\n        self.activity_encoder = LabelEncoder()\n\n        # Encode labels\n        self.encoded_services = self.service_encoder.fit_transform(\n            self.training_df['service']\n        )\n        self.encoded_activities = self.activity_encoder.fit_transform(\n            self.training_df['activityType']\n        )\n\n        # Prepare text features with technical context\n        self.texts = self.training_df.apply(\n            self._prepare_text_features,\n            axis=1\n        )\n\n    def _prepare_text_features(self, row):\n        # Enhanced feature extraction with technical context\n        technical_features = [\n            str(row.get('url', '')),\n            str(row.get('method', '')),\n            str(row.get('headers_Host', '')),\n            str(row.get('requestHeaders_Content_Type', '')),\n            str(row.get('responseHeaders_Content_Type', ''))\n        ]\n\n        # Join features, limit length\n        return \" \".join(technical_features)[:512]\n\n    def train(\n        self,\n        test_size=0.2,\n        batch_size=32,\n        epochs=10,\n        learning_rate=2e-5\n    ):\n        # Split data\n        (train_texts, val_texts,\n         train_service_labels, val_service_labels,\n         train_activity_labels, val_activity_labels) = train_test_split(\n            self.texts,\n            self.encoded_services,\n            self.encoded_activities,\n            test_size=test_size,\n            random_state=42\n        )\n\n        # Create datasets\n        train_dataset = CodeBertDataset(\n            train_texts,\n            train_service_labels,\n            train_activity_labels,\n            self.tokenizer\n        )\n        val_dataset = CodeBertDataset(\n            val_texts,\n            val_service_labels,\n            val_activity_labels,\n            self.tokenizer\n        )\n\n        # DataLoaders with optimization\n        train_loader = DataLoader(\n            train_dataset,\n            batch_size=batch_size,\n            shuffle=True,\n            pin_memory=True,\n            num_workers=2\n        )\n        val_loader = DataLoader(\n            val_dataset,\n            batch_size=batch_size,\n            pin_memory=True,\n            num_workers=2\n        )\n\n        # Model initialization\n        service_num_labels = len(self.service_encoder.classes_)\n        activity_num_labels = len(self.activity_encoder.classes_)\n\n        model = CodeBertTransformer(\n            service_num_labels,\n            activity_num_labels\n        ).to(self.device)\n\n        # Loss and optimizer\n        service_criterion = nn.CrossEntropyLoss()\n        activity_criterion = nn.CrossEntropyLoss()\n        optimizer = optim.AdamW(\n            model.parameters(),\n            lr=learning_rate,\n            weight_decay=0.01\n        )\n\n        # Learning rate scheduler\n        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n            optimizer,\n            mode='max',\n            factor=0.5,\n            patience=2\n        )\n\n        # Training loop\n        best_val_accuracy = 0\n        for epoch in range(epochs):\n            model.train()\n            total_train_loss = 0\n\n            for batch in train_loader:\n                optimizer.zero_grad()\n\n                input_ids = batch['input_ids'].to(self.device)\n                attention_mask = batch['attention_mask'].to(self.device)\n                service_labels = batch['service_label'].to(self.device)\n                activity_labels = batch['activity_label'].to(self.device)\n\n                service_pred, activity_pred = model(\n                    input_ids, attention_mask\n                )\n\n                service_loss = service_criterion(\n                    service_pred, service_labels\n                )\n                activity_loss = activity_criterion(\n                    activity_pred, activity_labels\n                )\n\n                total_loss = service_loss + activity_loss\n                total_loss.backward()\n                optimizer.step()\n\n                total_train_loss += total_loss.item()\n\n            # Validation phase\n            model.eval()\n            val_service_preds, val_activity_preds = [], []\n            val_service_true, val_activity_true = [], []\n\n            with torch.no_grad():\n                for batch in val_loader:\n                    input_ids = batch['input_ids'].to(self.device)\n                    attention_mask = batch['attention_mask'].to(self.device)\n\n                    service_pred, activity_pred = model(\n                        input_ids, attention_mask\n                    )\n\n                    val_service_preds.extend(\n                        torch.argmax(service_pred, dim=1).cpu().numpy()\n                    )\n                    val_activity_preds.extend(\n                        torch.argmax(activity_pred, dim=1).cpu().numpy()\n                    )\n\n                    val_service_true.extend(batch['service_label'].numpy())\n                    val_activity_true.extend(batch['activity_label'].numpy())\n\n            # Accuracy calculation\n            service_accuracy = np.mean(\n                np.array(val_service_preds) == np.array(val_service_true)\n            )\n            activity_accuracy = np.mean(\n                np.array(val_activity_preds) == np.array(val_activity_true)\n            )\n\n            print(f\"Epoch {epoch+1}: \"\n                  f\"Service Accuracy: {service_accuracy:.4f}, \"\n                  f\"Activity Accuracy: {activity_accuracy:.4f}\")\n\n            # Update learning rate\n            scheduler.step(service_accuracy + activity_accuracy)\n\n            # Save best model\n            current_accuracy = service_accuracy + activity_accuracy\n            if current_accuracy > best_val_accuracy:\n                best_val_accuracy = current_accuracy\n                torch.save(model.state_dict(), 'best_codebert_model.pth')\n                print(\"Saved best model\")\n\n        return model\n\ndef main():\n    training_data_path = '/kaggle/input/network-dataset/shuffled_train.csv'\n\n    try:\n        # Initialize and train classifier\n        classifier = CodeBertClassifier(training_data_path)\n        model = classifier.train()\n\n        print(\"Training completed successfully!\")\n\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        import traceback\n        traceback.print_exc()\n\nif __name__ == \"__main__\":\n    main()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-26T09:43:26.993451Z","iopub.execute_input":"2024-12-26T09:43:26.993709Z","iopub.status.idle":"2024-12-26T09:51:20.059344Z","shell.execute_reply.started":"2024-12-26T09:43:26.993688Z","shell.execute_reply":"2024-12-26T09:51:20.057814Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2269539dd64d49409d29cc25bdfb1193"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/498 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d4d16298584d474a93db19193f858439"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e9846d456e0a4ca78b79fd5204586665"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"01285b964be64c4abd1e523da6a1c44a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/150 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"60ad5369830d4018a79a23d6b43ee285"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/499M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a938e1b995a4e0485822781d73ec16c"}},"metadata":{}},{"name":"stdout","text":"Epoch 1: Service Accuracy: 0.9729, Activity Accuracy: 0.9910\nSaved best model\nEpoch 2: Service Accuracy: 0.9785, Activity Accuracy: 0.9958\nSaved best model\nEpoch 3: Service Accuracy: 0.9771, Activity Accuracy: 0.9986\nSaved best model\nEpoch 4: Service Accuracy: 0.9833, Activity Accuracy: 0.9986\nSaved best model\nEpoch 5: Service Accuracy: 0.9861, Activity Accuracy: 0.9986\nSaved best model\nEpoch 6: Service Accuracy: 0.9833, Activity Accuracy: 0.9979\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-5cfde14ff7ae>\u001b[0m in \u001b[0;36m<cell line: 328>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-2-5cfde14ff7ae>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0;31m# Initialize and train classifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0mclassifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCodeBertClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training completed successfully!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-2-5cfde14ff7ae>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, test_size, batch_size, epochs, learning_rate)\u001b[0m\n\u001b[1;32m    261\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m                 \u001b[0mtotal_train_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtotal_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m             \u001b[0;31m# Validation phase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":2},{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport pandas as pd\nimport numpy as np\nfrom transformers import AutoTokenizer, AutoModel\nfrom sklearn.preprocessing import LabelEncoder\n\nclass CodeBertTransformer(nn.Module):\n    def __init__(\n        self,\n        service_num_labels,\n        activity_num_labels,\n        model_name=\"microsoft/codebert-base\"\n    ):\n        super().__init__()\n\n        # Load CodeBERT model\n        self.transformer = AutoModel.from_pretrained(model_name)\n\n        # Dropout for regularization\n        self.dropout = nn.Dropout(0.3)\n\n        # Advanced classification heads\n        self.service_classifier = nn.Sequential(\n            nn.Linear(self.transformer.config.hidden_size, 512),\n            nn.BatchNorm1d(512),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(512, service_num_labels)\n        )\n\n        self.activity_classifier = nn.Sequential(\n            nn.Linear(self.transformer.config.hidden_size, 512),\n            nn.BatchNorm1d(512),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(512, activity_num_labels)\n        )\n\n    def forward(self, input_ids, attention_mask):\n        # Efficient forward pass\n        outputs = self.transformer(\n            input_ids=input_ids,\n            attention_mask=attention_mask\n        )\n\n        # Use CLS token representation\n        pooled_output = outputs.last_hidden_state[:, 0, :]\n        pooled_output = self.dropout(pooled_output)\n\n        # Classification\n        service_pred = self.service_classifier(pooled_output)\n        activity_pred = self.activity_classifier(pooled_output)\n\n        return service_pred, activity_pred\n\nclass CodeBertPredictor:\n    def __init__(\n        self,\n        model_path,\n        training_data_path,\n        model_name=\"microsoft/codebert-base\"\n    ):\n        # Device configuration\n        self.device = torch.device(\n            'cuda' if torch.cuda.is_available() else 'cpu'\n        )\n        print(f\"Using device: {self.device}\")\n\n        # Load training data for label encoding\n        self.training_df = pd.read_csv(\n            training_data_path,\n            low_memory=False\n        )\n\n        # Prepare data and tokenizer\n        self._prepare_data(model_name)\n\n        # Load trained model\n        self._load_model(model_path)\n\n    def _prepare_data(self, model_name):\n        # Validate and clean data\n        self.training_df['service'] = self.training_df['service'].fillna('Unknown')\n        self.training_df['activityType'] = self.training_df['activityType'].fillna('Unknown')\n\n        # Tokenizer\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n\n        # Label encoding\n        self.service_encoder = LabelEncoder()\n        self.activity_encoder = LabelEncoder()\n\n        # Fit encoders on training data\n        self.service_encoder.fit(self.training_df['service'])\n        self.activity_encoder.fit(self.training_df['activityType'])\n\n    def _load_model(self, model_path):\n        # Model initialization\n        service_num_labels = len(self.service_encoder.classes_)\n        activity_num_labels = len(self.activity_encoder.classes_)\n\n        # Create model\n        self.model = CodeBertTransformer(\n            service_num_labels,\n            activity_num_labels\n        ).to(self.device)\n\n        # Load trained weights\n        self.model.load_state_dict(torch.load(model_path, map_location=self.device))\n        self.model.eval()  # Set to evaluation mode\n\n    def _prepare_text_features(self, row):\n        # Enhanced feature extraction with technical context\n        technical_features = [\n            str(row.get('url', '')),\n            str(row.get('method', '')),\n            str(row.get('headers_Host', '')),\n            str(row.get('requestHeaders_Content_Type', '')),\n            str(row.get('responseHeaders_Content_Type', ''))\n        ]\n\n        # Join features, limit length\n        return \" \".join(technical_features)[:512]\n\n    def predict(self, test_df, confidence_threshold=0.5):\n        # Prepare text features\n        test_texts = test_df.apply(self._prepare_text_features, axis=1)\n\n        # Tokenize test data\n        encodings = self.tokenizer(\n            test_texts.tolist(),\n            truncation=True,\n            padding=True,\n            max_length=128,\n            return_tensors='pt'\n        )\n\n        # Prediction results\n        predictions = {\n            'predicted_service': [],\n            'service_confidence': [],\n            'predicted_activity': [],\n            'activity_confidence': []\n        }\n\n        # Disable gradient computation\n        with torch.no_grad():\n            # Move data to device\n            input_ids = encodings['input_ids'].to(self.device)\n            attention_mask = encodings['attention_mask'].to(self.device)\n\n            # Forward pass\n            service_pred, activity_pred = self.model(\n                input_ids, attention_mask\n            )\n\n            # Apply softmax to get probabilities\n            service_probs = F.softmax(service_pred, dim=1)\n            activity_probs = F.softmax(activity_pred, dim=1)\n\n            # Get top predictions and confidences\n            service_max_probs, service_preds = torch.max(service_probs, dim=1)\n            activity_max_probs, activity_preds = torch.max(activity_probs, dim=1)\n\n            # Convert to numpy for easier processing\n            service_preds = service_preds.cpu().numpy()\n            service_max_probs = service_max_probs.cpu().numpy()\n            activity_preds = activity_preds.cpu().numpy()\n            activity_max_probs = activity_max_probs.cpu().numpy()\n\n            # Decode predictions for ALL entries\n            for i in range(len(service_preds)):\n                # Decode labels\n                service = self.service_encoder.inverse_transform([service_preds[i]])[0]\n                activity = self.activity_encoder.inverse_transform([activity_preds[i]])[0]\n\n                # Check confidence threshold\n                if (service_max_probs[i] < confidence_threshold or\n                    activity_max_probs[i] < confidence_threshold):\n                    # If below threshold, mark as Unknown\n                    service = 'Unknown'\n                    activity = 'Unknown'\n\n                predictions['predicted_service'].append(service)\n                predictions['service_confidence'].append(float(service_max_probs[i]))\n                predictions['predicted_activity'].append(activity)\n                predictions['activity_confidence'].append(float(activity_max_probs[i]))\n\n        # Add predictions to original DataFrame\n        test_df['predicted_service'] = predictions['predicted_service']\n        test_df['service_confidence'] = predictions['service_confidence']\n        test_df['predicted_activity'] = predictions['predicted_activity']\n        test_df['activity_confidence'] = predictions['activity_confidence']\n\n        return test_df\n\n    def save_predictions(self, predictions_df, output_path='predictions.csv'):\n        \"\"\"Save predictions to a CSV file\"\"\"\n        predictions_df.to_csv(output_path, index=False)\n        print(f\"Predictions saved to {output_path}\")\n\ndef main():\n    # Paths\n    training_data_path = '/kaggle/input/network-dataset/shuffled_train.csv'\n    test_data_path = '/kaggle/input/network-dataset/shuffled_test.csv'\n    model_path = 'best_codebert_model.pth'\n    output_path = 'test_data_with_predictions_code_bert_1.csv'\n\n    try:\n        # Load test data\n        test_df = pd.read_csv(test_data_path, low_memory=False)\n\n        # Initialize predictor\n        predictor = CodeBertPredictor(\n            model_path=model_path,\n            training_data_path=training_data_path\n        )\n\n        # Predict with confidence threshold\n        predictions_df = predictor.predict(\n            test_df,\n            confidence_threshold=0.5  # Adjust as needed\n        )\n\n        # Print predictions summary\n        print(\"\\nPrediction Summary:\")\n        print(predictions_df[['predicted_service', 'service_confidence', 'predicted_activity', 'activity_confidence']].head())\n\n        # Save predictions\n        predictor.save_predictions(predictions_df, output_path)\n\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        import traceback\n        traceback.print_exc()\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T10:09:35.218218Z","iopub.execute_input":"2024-12-26T10:09:35.218555Z","iopub.status.idle":"2024-12-26T10:09:40.008352Z","shell.execute_reply.started":"2024-12-26T10:09:35.218530Z","shell.execute_reply":"2024-12-26T10:09:40.007380Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n<ipython-input-11-03f4a48cd9c0>:112: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  self.model.load_state_dict(torch.load(model_path, map_location=self.device))\n","output_type":"stream"},{"name":"stdout","text":"\nPrediction Summary:\n  predicted_service  service_confidence predicted_activity  \\\n0          OneDrive            0.985593              Login   \n1           4shared            0.992092             Upload   \n2         MediaFire            0.994190             Upload   \n3           Dropbox            0.994486             Upload   \n4           4shared            0.989075           Download   \n\n   activity_confidence  \n0             0.997256  \n1             0.996235  \n2             0.996443  \n3             0.995549  \n4             0.995101  \nPredictions saved to test_data_with_predictions_code_bert_1.csv\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport pandas as pd\nimport numpy as np\nfrom transformers import AutoTokenizer, AutoModel\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport re\n\nclass CodeBertTransformer(nn.Module):\n    def __init__(\n        self,\n        service_num_labels,\n        activity_num_labels,\n        model_name=\"microsoft/codebert-base\"\n    ):\n        super().__init__()\n\n        # Load CodeBERT model\n        self.transformer = AutoModel.from_pretrained(model_name)\n\n        # Dropout for regularization\n        self.dropout = nn.Dropout(0.3)\n\n        # Advanced classification heads\n        self.service_classifier = nn.Sequential(\n            nn.Linear(self.transformer.config.hidden_size, 512),\n            nn.BatchNorm1d(512),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(512, service_num_labels)\n        )\n\n        self.activity_classifier = nn.Sequential(\n            nn.Linear(self.transformer.config.hidden_size, 512),\n            nn.BatchNorm1d(512),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(512, activity_num_labels)\n        )\n\n    def forward(self, input_ids, attention_mask):\n        # Efficient forward pass\n        outputs = self.transformer(\n            input_ids=input_ids,\n            attention_mask=attention_mask\n        )\n\n        # Use CLS token representation\n        pooled_output = outputs.last_hidden_state[:, 0, :]\n        pooled_output = self.dropout(pooled_output)\n\n        # Classification\n        service_pred = self.service_classifier(pooled_output)\n        activity_pred = self.activity_classifier(pooled_output)\n\n        return service_pred, activity_pred, pooled_output  # Return embeddings too\n\nclass CodeBertPredictor:\n    def __init__(\n        self,\n        model_path,\n        training_data_path,\n        model_name=\"microsoft/codebert-base\"\n    ):\n        # Device configuration\n        self.device = torch.device(\n            'cuda' if torch.cuda.is_available() else 'cpu'\n        )\n        print(f\"Using device: {self.device}\")\n\n        # Comprehensive activity labels with semantic hierarchy\n        self.predefined_activities = [\n            # Primary action types\n            \"Login\", \"Logout\", \"Access\", \"Create\", \"Update\", \"Delete\",\n            \"View\", \"Edit\", \"Share\", \"Download\", \"Upload\",\n\n            # Secondary action types\n            \"Sync\", \"Connect\", \"Disconnect\", \"Authorize\",\n            \"Request\", \"Attempt\", \"Validate\",\n\n            # Anomaly and special states\n            \"Timeout\", \"Anomaly\", \"Error\", \"Suspend\", \"Resume\"\n        ]\n\n        # Semantic similarity matrix for activity mapping\n        self.activity_mapping = {\n            # Mapping ONLY unknown or vague terms to more specific labels\n            \"unknown\": [\"Attempt\", \"Access\", \"Request\", \"Unknown\"]\n        }\n\n        # Load training data for label encoding\n        self.training_df = pd.read_csv(\n            training_data_path,\n            low_memory=False\n        )\n\n        # Prepare data and tokenizer\n        self._prepare_data(model_name)\n\n        # Load trained model\n        self._load_model(model_path)\n\n    def _prepare_data(self, model_name):\n        # Validate and clean data\n        self.training_df['service'] = self.training_df['service'].fillna('Unknown')\n        self.training_df['activityType'] = self.training_df['activityType'].fillna('Unknown')\n\n        # Tokenizer\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n\n        # Label encoding\n        self.service_encoder = LabelEncoder()\n        self.activity_encoder = LabelEncoder()\n\n        # Fit encoders on training data\n        self.service_encoder.fit(self.training_df['service'])\n        self.activity_encoder.fit(self.training_df['activityType'])\n\n    def _load_model(self, model_path):\n        # Model initialization\n        service_num_labels = len(self.service_encoder.classes_)\n        activity_num_labels = len(self.activity_encoder.classes_)\n\n        # Create model\n        self.model = CodeBertTransformer(\n            service_num_labels,\n            activity_num_labels\n        ).to(self.device)\n\n        # Load trained weights\n        self.model.load_state_dict(torch.load(model_path, map_location=self.device))\n        self.model.eval()  # Set to evaluation mode\n\n    def _prepare_text_features(self, row):\n        # Enhanced feature extraction with technical context\n        technical_features = [\n            str(row.get('url', '')),\n            str(row.get('method', '')),\n            str(row.get('headers_Host', '')),\n            str(row.get('requestHeaders_Content_Type', '')),\n            str(row.get('responseHeaders_Content_Type', ''))\n        ]\n\n        # Join features, limit length\n        return \" \".join(technical_features)[:512]\n\n    def _advanced_activity_mapping(self, original_activity):\n        \"\"\"\n        Advanced mapping for activity types\n\n        Args:\n            original_activity (str): Original activity type\n\n        Returns:\n            tuple: (mapped_activity, confidence_score)\n        \"\"\"\n        # Normalize input\n        norm_activity = str(original_activity).lower().strip()\n\n        # Exact match first\n        if norm_activity in [a.lower() for a in self.predefined_activities]:\n            return original_activity, 1.0\n\n        # Check ONLY unknown mapping\n        if norm_activity in ['unknown', 'unspecified', '']:\n            for mapping in self.activity_mapping.get('unknown', ['Access']):\n                return mapping, 0.7\n\n        # Text-based pattern matching for truly unknown cases\n        pattern_mapping = [\n            (r'^unknown$|^unspecified$|^\\s*$', 'Access'),\n            (r'generic|undefined|null', 'Request')\n        ]\n\n        for pattern, activity in pattern_mapping:\n            if re.search(pattern, norm_activity, re.IGNORECASE):\n                return activity, 0.5\n\n        # Fallback to most generic activity\n        return \"Access\", 0.3\n\n    def _find_closest_activity(self, embedding):\n        \"\"\"\n        Find the closest activity label using cosine similarity\n\n        Args:\n            embedding (torch.Tensor): Input embedding\n\n        Returns:\n            tuple: (closest activity, similarity score)\n        \"\"\"\n        # Tokenize predefined activities\n        activity_encodings = self.tokenizer(\n            self.predefined_activities,\n            truncation=True,\n            padding=True,\n            max_length=128,\n            return_tensors='pt'\n        )\n\n        # Compute embeddings for predefined activities\n        with torch.no_grad():\n            activity_input_ids = activity_encodings['input_ids'].to(self.device)\n            activity_attention_mask = activity_encodings['attention_mask'].to(self.device)\n\n            # Get embeddings for predefined activities\n            _, _, activity_embeddings = self.model(activity_input_ids, activity_attention_mask)\n\n        # Convert embeddings to numpy for cosine similarity\n        input_embedding_np = embedding.cpu().numpy().reshape(1, -1)\n        activity_embeddings_np = activity_embeddings.cpu().numpy()\n\n        # Compute cosine similarities\n        similarities = cosine_similarity(input_embedding_np, activity_embeddings_np)[0]\n\n        # Find the index of the most similar activity\n        closest_idx = np.argmax(similarities)\n        closest_similarity = similarities[closest_idx]\n\n        # Return closest activity with its similarity score\n        return self.predefined_activities[closest_idx], closest_similarity\n\n    def predict(self, test_df, confidence_threshold=0.5, semantic_threshold=0.3):\n        # Prepare text features\n        test_texts = test_df.apply(self._prepare_text_features, axis=1)\n\n        # Tokenize test data\n        encodings = self.tokenizer(\n            test_texts.tolist(),\n            truncation=True,\n            padding=True,\n            max_length=128,\n            return_tensors='pt'\n        )\n\n        # Prediction results\n        predictions = {\n            'predicted_service': [],\n            'service_confidence': [],\n            'predicted_activity': [],\n            'activity_confidence': []\n        }\n\n        # Disable gradient computation\n        with torch.no_grad():\n            # Move data to device\n            input_ids = encodings['input_ids'].to(self.device)\n            attention_mask = encodings['attention_mask'].to(self.device)\n\n            # Forward pass\n            service_pred, activity_pred, embeddings = self.model(\n                input_ids, attention_mask\n            )\n\n            # Apply softmax to get probabilities\n            service_probs = F.softmax(service_pred, dim=1)\n            activity_probs = F.softmax(activity_pred, dim=1)\n\n            # Get top predictions and confidences\n            service_max_probs, service_preds = torch.max(service_probs, dim=1)\n            activity_max_probs, activity_preds = torch.max(activity_probs, dim=1)\n\n            # Convert to numpy for easier processing\n            service_preds = service_preds.cpu().numpy()\n            service_max_probs = service_max_probs.cpu().numpy()\n            activity_preds = activity_preds.cpu().numpy()\n            activity_max_probs = activity_max_probs.cpu().numpy()\n\n            # Decode predictions for ALL entries\n            for i in range(len(service_preds)):\n                # Decode labels\n                service = self.service_encoder.inverse_transform([service_preds[i]])[0]\n                original_activity = self.activity_encoder.inverse_transform([activity_preds[i]])[0]\n\n                # Normalize original activity\n                norm_original_activity = str(original_activity).lower().strip()\n\n                # Check if activity is truly unknown or not in predefined list\n                if (norm_original_activity == 'unknown' or\n                    norm_original_activity not in [a.lower() for a in self.predefined_activities]):\n                    # Try semantic matching for truly unknown activities\n                    semantic_activity, semantic_score = self._find_closest_activity(embeddings[i])\n\n                    # Use semantic match if above threshold\n                    if semantic_score >= semantic_threshold:\n                        mapped_activity = semantic_activity\n                        confidence = semantic_score\n                    else:\n                        # Fallback to advanced mapping\n                        mapped_activity, confidence = self._advanced_activity_mapping(original_activity)\n                else:\n                    # For known activities, use original prediction\n                    mapped_activity = original_activity\n                    confidence = float(activity_max_probs[i])\n\n                # Adjust confidence if below threshold\n                if confidence < confidence_threshold:\n                    mapped_activity = \"Unknown\"\n                    confidence = 0.1  # Very low confidence\n\n                predictions['predicted_service'].append(service)\n                predictions['service_confidence'].append(float(service_max_probs[i]))\n                predictions['predicted_activity'].append(mapped_activity)\n                predictions['activity_confidence'].append(confidence)\n\n        # Add predictions to original DataFrame\n        test_df['predicted_service'] = predictions['predicted_service']\n        test_df['service_confidence'] = predictions['service_confidence']\n        test_df['predicted_activity'] = predictions['predicted_activity']\n        test_df['activity_confidence'] = predictions['activity_confidence']\n\n        return test_df\n\n    def save_predictions(self, predictions_df, output_path='predictions.csv'):\n        \"\"\"Save predictions to a CSV file\"\"\"\n        predictions_df.to_csv(output_path, index=False)\n        print(f\"Predictions saved to {output_path}\")\n\ndef main():\n    # Paths\n    training_data_path = '/kaggle/input/network-dataset/shuffled_train.csv'\n    test_data_path = '/kaggle/input/network-dataset/shuffled_test.csv'\n    model_path = 'best_codebert_model.pth'\n    output_path = 'test_data_with_predictions_code_bert_2.csv'\n\n    try:\n        # Load test data\n        test_df = pd.read_csv(test_data_path, low_memory=False)\n\n        # Initialize predictor\n        predictor = CodeBertPredictor(\n            model_path=model_path,\n            training_data_path=training_data_path\n        )\n\n        # Predict with confidence threshold\n        predictions_df = predictor.predict(\n            test_df,\n            confidence_threshold=0.5,  # Model confidence threshold\n            semantic_threshold=0.3     # Semantic matching threshold\n        )\n\n        # Print predictions summary\n        print(\"\\nPrediction Summary:\")\n        print(predictions_df[['predicted_service', 'service_confidence', 'predicted_activity', 'activity_confidence']].head())\n\n        # Save predictions\n        predictor.save_predictions(predictions_df, output_path)\n\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        import traceback\n        traceback.print_exc()\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T10:09:50.994879Z","iopub.execute_input":"2024-12-26T10:09:50.995224Z","iopub.status.idle":"2024-12-26T10:10:01.659486Z","shell.execute_reply.started":"2024-12-26T10:09:50.995196Z","shell.execute_reply":"2024-12-26T10:10:01.658150Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n<ipython-input-12-d05f8c71eab5>:134: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  self.model.load_state_dict(torch.load(model_path, map_location=self.device))\n","output_type":"stream"},{"name":"stdout","text":"\nPrediction Summary:\n  predicted_service  service_confidence predicted_activity  \\\n0          OneDrive            0.985593              Login   \n1           4shared            0.992092             Upload   \n2         MediaFire            0.994190             Upload   \n3           Dropbox            0.994486             Upload   \n4           4shared            0.989075           Download   \n\n   activity_confidence  \n0             0.997256  \n1             0.996235  \n2             0.996443  \n3             0.995549  \n4             0.995101  \nPredictions saved to test_data_with_predictions_code_bert_2.csv\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport pandas as pd\nimport numpy as np\nfrom transformers import AutoTokenizer, AutoModel\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nclass ZeroShotActivityPredictor:\n    def __init__(self, activity_labels, model_name=\"microsoft/codebert-base\"):\n        # Load the model and tokenizer for ZSL\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        self.model = AutoModel.from_pretrained(model_name)\n        \n        # Define activity labels for zero-shot classification\n        self.activity_labels = activity_labels\n        self.activity_embeddings = self._get_activity_embeddings()\n\n    def _get_activity_embeddings(self):\n        \"\"\"Generate embeddings for predefined activities to be used for similarity comparison\"\"\"\n        activity_embeddings = {}\n        for activity in self.activity_labels:\n            # Tokenize and encode each activity label\n            inputs = self.tokenizer(activity, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n            with torch.no_grad():\n                outputs = self.model(**inputs)\n            # Use [CLS] token embedding for activity representation\n            activity_embeddings[activity] = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n        return activity_embeddings\n\n    def predict(self, activity_text):\n        \"\"\"Predict the activity for an unknown label using ZSL\"\"\"\n        # Tokenize the unknown activity text\n        inputs = self.tokenizer(activity_text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n        with torch.no_grad():\n            outputs = self.model(**inputs)\n        # Use [CLS] token embedding for the activity\n        activity_embedding = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n\n        # Calculate cosine similarities with predefined activity embeddings\n        similarities = {}\n        for activity, embedding in self.activity_embeddings.items():\n            sim = cosine_similarity(activity_embedding, embedding)\n            similarities[activity] = sim\n\n        # Find the most similar predefined activity\n        best_match = max(similarities, key=similarities.get)\n        return best_match, similarities[best_match]\n\nclass CodeBertTransformer(nn.Module):\n    def __init__(self, service_num_labels, activity_num_labels, model_name=\"microsoft/codebert-base\"):\n        super().__init__()\n\n        # Load CodeBERT model\n        self.transformer = AutoModel.from_pretrained(model_name)\n\n        # Dropout for regularization\n        self.dropout = nn.Dropout(0.3)\n\n        # Advanced classification heads\n        self.service_classifier = nn.Sequential(\n            nn.Linear(self.transformer.config.hidden_size, 512),\n            nn.BatchNorm1d(512),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(512, service_num_labels)\n        )\n\n        self.activity_classifier = nn.Sequential(\n            nn.Linear(self.transformer.config.hidden_size, 512),\n            nn.BatchNorm1d(512),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(512, activity_num_labels)\n        )\n\n    def forward(self, input_ids, attention_mask):\n        # Efficient forward pass\n        outputs = self.transformer(\n            input_ids=input_ids,\n            attention_mask=attention_mask\n        )\n\n        # Use CLS token representation\n        pooled_output = outputs.last_hidden_state[:, 0, :]\n        pooled_output = self.dropout(pooled_output)\n\n        # Classification\n        service_pred = self.service_classifier(pooled_output)\n        activity_pred = self.activity_classifier(pooled_output)\n\n        return service_pred, activity_pred, pooled_output  # Return embeddings too\n\nclass CodeBertPredictor:\n    def __init__(self, model_path, training_data_path, model_name=\"microsoft/codebert-base\"):\n        # Device configuration\n        self.device = torch.device(\n            'cuda' if torch.cuda.is_available() else 'cpu'\n        )\n        print(f\"Using device: {self.device}\")\n\n        # Predefined activities\n        self.predefined_activities = [\n            \"Login\", \"Logout\", \"Access\", \"Create\", \"Update\", \"Delete\",\n            \"View\", \"Edit\", \"Share\", \"Download\", \"Upload\",\n            \"Request\", \"Timeout\", \"Error\"\n        ]\n\n        # Load training data for label encoding\n        self.training_df = pd.read_csv(\n            training_data_path,\n            low_memory=False\n        )\n\n        # Prepare data and tokenizer\n        self._prepare_data(model_name)\n\n        # Load trained model\n        self._load_model(model_path)\n\n    def _prepare_data(self, model_name):\n        # Validate and clean data\n        self.training_df['service'] = self.training_df['service'].fillna('Unknown')\n        self.training_df['activityType'] = self.training_df['activityType'].fillna('Unknown')\n\n        # Tokenizer\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n\n        # Label encoding\n        self.service_encoder = LabelEncoder()\n        self.activity_encoder = LabelEncoder()\n\n        # Fit encoders on training data\n        self.service_encoder.fit(self.training_df['service'])\n        self.activity_encoder.fit(self.training_df['activityType'])\n\n    def _load_model(self, model_path):\n        # Model initialization\n        service_num_labels = len(self.service_encoder.classes_)\n        activity_num_labels = len(self.activity_encoder.classes_)\n\n        # Create model\n        self.model = CodeBertTransformer(\n            service_num_labels,\n            activity_num_labels\n        ).to(self.device)\n\n        # Load trained weights\n        self.model.load_state_dict(torch.load(model_path, map_location=self.device))\n        self.model.eval()  # Set to evaluation mode\n\n    def _prepare_text_features(self, row):\n        # Enhanced feature extraction with technical context\n        technical_features = [\n            str(row.get('url', '')),\n            str(row.get('method', '')),\n            str(row.get('headers_Host', '')),\n            str(row.get('requestHeaders_Content_Type', '')),\n            str(row.get('responseHeaders_Content_Type', ''))\n        ]\n\n        # Join features, limit length\n        return \" \".join(technical_features)[:512]\n\n    def predict(self, test_df):\n        # Prepare text features\n        test_texts = test_df.apply(self._prepare_text_features, axis=1)\n\n        # Tokenize test data\n        encodings = self.tokenizer(\n            test_texts.tolist(),\n            truncation=True,\n            padding=True,\n            max_length=128,\n            return_tensors='pt'\n        )\n\n        # Prediction results\n        predictions = {\n            'predicted_service': [],\n            'service_confidence': [],\n            'predicted_activity': [],\n            'activity_confidence': []\n        }\n\n        # Disable gradient computation\n        with torch.no_grad():\n            # Move data to device\n            input_ids = encodings['input_ids'].to(self.device)\n            attention_mask = encodings['attention_mask'].to(self.device)\n\n            # Forward pass\n            service_pred, activity_pred, embeddings = self.model(\n                input_ids, attention_mask\n            )\n\n            # Apply softmax to get probabilities\n            service_probs = F.softmax(service_pred, dim=1)\n            activity_probs = F.softmax(activity_pred, dim=1)\n\n            # Get top predictions and confidences\n            service_max_probs, service_preds = torch.max(service_probs, dim=1)\n            activity_max_probs, activity_preds = torch.max(activity_probs, dim=1)\n\n            # Convert to numpy for easier processing\n            service_preds = service_preds.cpu().numpy()\n            service_max_probs = service_max_probs.cpu().numpy()\n            activity_preds = activity_preds.cpu().numpy()\n            activity_max_probs = activity_max_probs.cpu().numpy()\n\n            # Decode predictions for all entries\n            for i in range(len(service_preds)):\n                # Decode labels\n                service = self.service_encoder.inverse_transform([service_preds[i]])[0]\n                original_activity = self.activity_encoder.inverse_transform([activity_preds[i]])[0]\n\n                # If activity is unknown, use ZSL to predict\n                if original_activity.lower() == 'unknown':\n                    mapped_activity, activity_confidence = self.zsl_model.predict(test_texts[i])\n                else:\n                    mapped_activity = original_activity\n                    activity_confidence = float(activity_max_probs[i])\n\n                # Ensure activity_confidence is a scalar before appending\n                if isinstance(activity_confidence, (torch.Tensor, np.ndarray)):\n                    activity_confidence = activity_confidence.item()  # Extract scalar value\n\n                predictions['predicted_service'].append(service)\n                predictions['service_confidence'].append(float(service_max_probs[i]))\n                predictions['predicted_activity'].append(mapped_activity)\n                predictions['activity_confidence'].append(float(activity_confidence))\n\n        # Add predictions to original DataFrame\n        test_df['predicted_service'] = predictions['predicted_service']\n        test_df['service_confidence'] = predictions['service_confidence']\n        test_df['predicted_activity'] = predictions['predicted_activity']\n        test_df['activity_confidence'] = predictions['activity_confidence']\n\n        return test_df\n\n    def save_predictions(self, predictions_df, output_path='predictions.csv'):\n        \"\"\"Save predictions to a CSV file\"\"\"\n        predictions_df.to_csv(output_path, index=False)\n        print(f\"Predictions saved to {output_path}\")\n\nclass CodeBertPredictorWithZSL(CodeBertPredictor):\n    def __init__(self, model_path, training_data_path, zsl_model, model_name=\"microsoft/codebert-base\"):\n        super().__init__(model_path, training_data_path, model_name)\n        # Initialize Zero-Shot Learner\n        self.zsl_model = zsl_model\n\n# Main Execution\ndef main():\n    # Paths\n    training_data_path = '/kaggle/input/network-dataset/shuffled_train.csv'\n    test_data_path = '/kaggle/input/network-dataset/shuffled_test.csv'\n    model_path = 'best_codebert_model.pth'\n    output_path = 'test_data_with_predictions_code_bert_zsl.csv'\n\n    try:\n        # Initialize Zero-Shot Activity Predictor\n        predefined_activities = [\n            \"Login\", \"Logout\", \"Access\", \"Create\", \"Update\", \"Delete\",\n            \"View\", \"Edit\", \"Share\", \"Download\", \"Upload\",\n            \"Request\", \"Timeout\", \"Error\"\n        ]\n        zsl_model = ZeroShotActivityPredictor(predefined_activities)\n\n        # Load test data\n        test_df = pd.read_csv(test_data_path, low_memory=False)\n\n        # Initialize predictor with ZSL\n        predictor = CodeBertPredictorWithZSL(\n            model_path=model_path,\n            training_data_path=training_data_path,\n            zsl_model=zsl_model\n        )\n\n        # Predict with confidence threshold\n        predictions_df = predictor.predict(\n            test_df\n        )\n\n        # Print predictions summary\n        print(\"\\nPrediction Summary:\")\n        print(predictions_df[['predicted_service', 'service_confidence', 'predicted_activity', 'activity_confidence']].head())\n\n        # Save predictions\n        predictor.save_predictions(predictions_df, output_path)\n\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        import traceback\n        traceback.print_exc()\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T10:12:21.072080Z","iopub.execute_input":"2024-12-26T10:12:21.072407Z","iopub.status.idle":"2024-12-26T10:12:48.754588Z","shell.execute_reply.started":"2024-12-26T10:12:21.072385Z","shell.execute_reply":"2024-12-26T10:12:48.753673Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n<ipython-input-14-5ddf04734183>:151: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  self.model.load_state_dict(torch.load(model_path, map_location=self.device))\n","output_type":"stream"},{"name":"stdout","text":"\nPrediction Summary:\n  predicted_service  service_confidence predicted_activity  \\\n0          OneDrive            0.985593              Login   \n1           4shared            0.992092             Upload   \n2         MediaFire            0.994190             Upload   \n3           Dropbox            0.994486             Upload   \n4           4shared            0.989075           Download   \n\n   activity_confidence  \n0             0.997256  \n1             0.996235  \n2             0.996443  \n3             0.995549  \n4             0.995101  \nPredictions saved to test_data_with_predictions_code_bert_zsl.csv\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport pandas as pd\nimport numpy as np\nfrom transformers import AutoTokenizer, AutoModel\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nclass ZeroShotActivityPredictor:\n    def __init__(self, activity_labels, model_name=\"microsoft/codebert-base\"):\n        # Load the model and tokenizer for ZSL\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        self.model = AutoModel.from_pretrained(model_name)\n        \n        # Define activity labels for zero-shot classification\n        self.activity_labels = activity_labels\n        self.activity_embeddings = self._get_activity_embeddings()\n\n    def _get_activity_embeddings(self):\n        \"\"\"Generate embeddings for predefined activities to be used for similarity comparison\"\"\"\n        activity_embeddings = {}\n        for activity in self.activity_labels:\n            # Tokenize and encode each activity label\n            inputs = self.tokenizer(activity, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n            with torch.no_grad():\n                outputs = self.model(**inputs)\n            # Use [CLS] token embedding for activity representation\n            activity_embeddings[activity] = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n        return activity_embeddings\n\n    def predict(self, activity_text):\n        \"\"\"Predict the activity for an unknown label using ZSL\"\"\"\n        # Tokenize the unknown activity text\n        inputs = self.tokenizer(activity_text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n        with torch.no_grad():\n            outputs = self.model(**inputs)\n        # Use [CLS] token embedding for the activity\n        activity_embedding = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n\n        # Calculate cosine similarities with predefined activity embeddings\n        similarities = {}\n        for activity, embedding in self.activity_embeddings.items():\n            sim = cosine_similarity(activity_embedding, embedding)\n            similarities[activity] = sim\n\n        # Find the most similar predefined activity\n        best_match = max(similarities, key=similarities.get)\n        return best_match, similarities[best_match]\n\nclass CodeBertTransformer(nn.Module):\n    def __init__(self, service_num_labels, activity_num_labels, model_name=\"microsoft/codebert-base\"):\n        super().__init__()\n\n        # Load CodeBERT model\n        self.transformer = AutoModel.from_pretrained(model_name)\n\n        # Dropout for regularization\n        self.dropout = nn.Dropout(0.3)\n\n        # Advanced classification heads\n        self.service_classifier = nn.Sequential(\n            nn.Linear(self.transformer.config.hidden_size, 512),\n            nn.BatchNorm1d(512),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(512, service_num_labels)\n        )\n\n        self.activity_classifier = nn.Sequential(\n            nn.Linear(self.transformer.config.hidden_size, 512),\n            nn.BatchNorm1d(512),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(512, activity_num_labels)\n        )\n\n    def forward(self, input_ids, attention_mask):\n        # Efficient forward pass\n        outputs = self.transformer(\n            input_ids=input_ids,\n            attention_mask=attention_mask\n        )\n\n        # Use CLS token representation\n        pooled_output = outputs.last_hidden_state[:, 0, :]\n        pooled_output = self.dropout(pooled_output)\n\n        # Classification\n        service_pred = self.service_classifier(pooled_output)\n        activity_pred = self.activity_classifier(pooled_output)\n\n        return service_pred, activity_pred, pooled_output  # Return embeddings too\n\nclass CodeBertPredictor:\n    def __init__(self, model_path, training_data_path, model_name=\"microsoft/codebert-base\"):\n        # Device configuration\n        self.device = torch.device(\n            'cuda' if torch.cuda.is_available() else 'cpu'\n        )\n        print(f\"Using device: {self.device}\")\n\n        # Predefined activities\n        self.predefined_activities = [\n            \"Login\", \"Logout\", \"Access\", \"Create\", \"Update\", \"Delete\",\n            \"View\", \"Edit\", \"Share\", \"Download\", \"Upload\",\n            \"Request\", \"Timeout\", \"Error\"\n        ]\n\n        # Load training data for label encoding\n        self.training_df = pd.read_csv(\n            training_data_path,\n            low_memory=False\n        )\n\n        # Prepare data and tokenizer\n        self._prepare_data(model_name)\n\n        # Load trained model\n        self._load_model(model_path)\n\n    def _prepare_data(self, model_name):\n        # Validate and clean data\n        self.training_df['service'] = self.training_df['service'].fillna('Unknown')\n        self.training_df['activityType'] = self.training_df['activityType'].fillna('Unknown')\n\n        # Tokenizer\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n\n        # Label encoding\n        self.service_encoder = LabelEncoder()\n        self.activity_encoder = LabelEncoder()\n\n        # Fit encoders on training data\n        self.service_encoder.fit(self.training_df['service'])\n        self.activity_encoder.fit(self.training_df['activityType'])\n\n    def _load_model(self, model_path):\n        # Model initialization\n        service_num_labels = len(self.service_encoder.classes_)\n        activity_num_labels = len(self.activity_encoder.classes_)\n\n        # Create model\n        self.model = CodeBertTransformer(\n            service_num_labels,\n            activity_num_labels\n        ).to(self.device)\n\n        # Load trained weights\n        self.model.load_state_dict(torch.load(model_path, map_location=self.device))\n        self.model.eval()  # Set to evaluation mode\n\n    def _prepare_text_features(self, row):\n        # Enhanced feature extraction with technical context\n        technical_features = [\n            str(row.get('url', '')),\n            str(row.get('method', '')),\n            str(row.get('headers_Host', '')),\n            str(row.get('requestHeaders_Content_Type', '')),\n            str(row.get('responseHeaders_Content_Type', ''))\n        ]\n\n        # Join features, limit length\n        return \" \".join(technical_features)[:512]\n\n    def predict(self, test_df):\n        # Prepare text features\n        test_texts = test_df.apply(self._prepare_text_features, axis=1)\n\n        # Tokenize test data\n        encodings = self.tokenizer(\n            test_texts.tolist(),\n            truncation=True,\n            padding=True,\n            max_length=128,\n            return_tensors='pt'\n        )\n\n        # Prediction results\n        predictions = {\n            'predicted_service': [],\n            'service_confidence': [],\n            'predicted_activity': [],\n            'activity_confidence': []\n        }\n\n        # Disable gradient computation\n        with torch.no_grad():\n            # Move data to device\n            input_ids = encodings['input_ids'].to(self.device)\n            attention_mask = encodings['attention_mask'].to(self.device)\n\n            # Forward pass\n            service_pred, activity_pred, embeddings = self.model(\n                input_ids, attention_mask\n            )\n\n            # Apply softmax to get probabilities\n            service_probs = F.softmax(service_pred, dim=1)\n            activity_probs = F.softmax(activity_pred, dim=1)\n\n            # Get top predictions and confidences\n            service_max_probs, service_preds = torch.max(service_probs, dim=1)\n            activity_max_probs, activity_preds = torch.max(activity_probs, dim=1)\n\n            # Convert to numpy for easier processing\n            service_preds = service_preds.cpu().numpy()\n            service_max_probs = service_max_probs.cpu().numpy()\n            activity_preds = activity_preds.cpu().numpy()\n            activity_max_probs = activity_max_probs.cpu().numpy()\n\n            # Decode predictions for all entries\n            for i in range(len(service_preds)):\n                # Decode labels\n                service = self.service_encoder.inverse_transform([service_preds[i]])[0]\n                original_activity = self.activity_encoder.inverse_transform([activity_preds[i]])[0]\n\n                # If activity is unknown, use ZSL to predict\n                if original_activity.lower() == 'unknown':\n                    mapped_activity, activity_confidence = self.zsl_model.predict(test_texts[i])\n                else:\n                    mapped_activity = original_activity\n                    activity_confidence = float(activity_max_probs[i])\n\n                # Ensure activity_confidence is a scalar before appending\n                if isinstance(activity_confidence, (torch.Tensor, np.ndarray)):\n                    activity_confidence = activity_confidence.item()  # Extract scalar value\n\n                predictions['predicted_service'].append(service)\n                predictions['service_confidence'].append(float(service_max_probs[i]))\n                predictions['predicted_activity'].append(mapped_activity)\n                predictions['activity_confidence'].append(float(activity_confidence))\n\n        # Add predictions to original DataFrame\n        test_df['predicted_service'] = predictions['predicted_service']\n        test_df['service_confidence'] = predictions['service_confidence']\n        test_df['predicted_activity'] = predictions['predicted_activity']\n        test_df['activity_confidence'] = predictions['activity_confidence']\n\n        return test_df\n\n    def save_predictions(self, predictions_df, output_path='predictions.csv'):\n        \"\"\"Save predictions to a CSV file\"\"\"\n        predictions_df.to_csv(output_path, index=False)\n        print(f\"Predictions saved to {output_path}\")\n\nclass CodeBertPredictorWithZSL(CodeBertPredictor):\n    def __init__(self, model_path, training_data_path, zsl_model, model_name=\"microsoft/codebert-base\"):\n        super().__init__(model_path, training_data_path, model_name)\n        # Initialize Zero-Shot Learner\n        self.zsl_model = zsl_model\n\n# Main Execution\ndef main():\n    # Paths\n    training_data_path = '/kaggle/input/network-dataset/shuffled_train.csv'\n    test_data_path = '/kaggle/input/network-dataset/shuffled_test.csv'\n    model_path = 'best_codebert_model.pth'\n    output_path = 'test_data_with_predictions_code_bert_zsl.csv'\n\n    try:\n        # Initialize Zero-Shot Activity Predictor\n        predefined_activities = [\n            \"Login\", \"Logout\", \"Access\", \"Create\", \"Update\", \"Delete\",\n            \"View\", \"Edit\", \"Share\", \"Download\", \"Upload\",\n            \"Request\", \"Timeout\", \"Error\"\n        ]\n        zsl_model = ZeroShotActivityPredictor(predefined_activities)\n\n        # Load test data\n        test_df = pd.read_csv(test_data_path, low_memory=False)\n\n        # Initialize predictor with ZSL\n        predictor = CodeBertPredictorWithZSL(\n            model_path=model_path,\n            training_data_path=training_data_path,\n            zsl_model=zsl_model\n        )\n\n        # Predict with confidence threshold\n        predictions_df = predictor.predict(\n            test_df\n        )\n\n        # Print predictions summary\n        print(\"\\nPrediction Summary:\")\n        print(predictions_df[['predicted_service', 'service_confidence', 'predicted_activity', 'activity_confidence']].head())\n\n        # Save predictions\n        predictor.save_predictions(predictions_df, output_path)\n\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        import traceback\n        traceback.print_exc()\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T10:15:13.344387Z","iopub.execute_input":"2024-12-26T10:15:13.344772Z","iopub.status.idle":"2024-12-26T10:15:40.542036Z","shell.execute_reply.started":"2024-12-26T10:15:13.344740Z","shell.execute_reply":"2024-12-26T10:15:40.541127Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n<ipython-input-16-5ddf04734183>:151: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  self.model.load_state_dict(torch.load(model_path, map_location=self.device))\n","output_type":"stream"},{"name":"stdout","text":"\nPrediction Summary:\n  predicted_service  service_confidence predicted_activity  \\\n0          OneDrive            0.985593              Login   \n1           4shared            0.992092             Upload   \n2         MediaFire            0.994190             Upload   \n3           Dropbox            0.994486             Upload   \n4           4shared            0.989075           Download   \n\n   activity_confidence  \n0             0.997256  \n1             0.996235  \n2             0.996443  \n3             0.995549  \n4             0.995101  \nPredictions saved to test_data_with_predictions_code_bert_zsl.csv\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport pandas as pd\nimport numpy as np\nfrom transformers import AutoTokenizer, AutoModel\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nclass ZeroShotActivityPredictor:\n    def __init__(self, activity_labels, model_name=\"microsoft/codebert-base\"):\n        # Load the model and tokenizer for ZSL\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        self.model = AutoModel.from_pretrained(model_name)\n        \n        # Define activity labels for zero-shot classification\n        self.activity_labels = activity_labels\n        self.activity_embeddings = self._get_activity_embeddings()\n\n    def _get_activity_embeddings(self):\n        \"\"\"Generate embeddings for predefined activities to be used for similarity comparison\"\"\"\n        activity_embeddings = {}\n        for activity in self.activity_labels:\n            # Tokenize and encode each activity label\n            inputs = self.tokenizer(activity, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n            with torch.no_grad():\n                outputs = self.model(**inputs)\n            # Use [CLS] token embedding for activity representation\n            activity_embeddings[activity] = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n        return activity_embeddings\n\n    def predict(self, activity_text):\n        \"\"\"Predict the activity for an unknown label using ZSL\"\"\"\n        # Tokenize the unknown activity text\n        inputs = self.tokenizer(activity_text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n        with torch.no_grad():\n            outputs = self.model(**inputs)\n        # Use [CLS] token embedding for the activity\n        activity_embedding = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n\n        # Calculate cosine similarities with predefined activity embeddings\n        similarities = {}\n        for activity, embedding in self.activity_embeddings.items():\n            sim = cosine_similarity(activity_embedding, embedding)\n            similarities[activity] = sim\n\n        # Find the most similar predefined activity\n        best_match = max(similarities, key=similarities.get)\n        return best_match, similarities[best_match]\n\nclass CodeBertTransformer(nn.Module):\n    def __init__(self, service_num_labels, activity_num_labels, model_name=\"microsoft/codebert-base\"):\n        super().__init__()\n\n        # Load CodeBERT model\n        self.transformer = AutoModel.from_pretrained(model_name)\n\n        # Dropout for regularization\n        self.dropout = nn.Dropout(0.3)\n\n        # Advanced classification heads\n        self.service_classifier = nn.Sequential(\n            nn.Linear(self.transformer.config.hidden_size, 512),\n            nn.BatchNorm1d(512),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(512, service_num_labels)\n        )\n\n        self.activity_classifier = nn.Sequential(\n            nn.Linear(self.transformer.config.hidden_size, 512),\n            nn.BatchNorm1d(512),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(512, activity_num_labels)\n        )\n\n    def forward(self, input_ids, attention_mask):\n        # Efficient forward pass\n        outputs = self.transformer(\n            input_ids=input_ids,\n            attention_mask=attention_mask\n        )\n\n        # Use CLS token representation\n        pooled_output = outputs.last_hidden_state[:, 0, :]\n        pooled_output = self.dropout(pooled_output)\n\n        # Classification\n        service_pred = self.service_classifier(pooled_output)\n        activity_pred = self.activity_classifier(pooled_output)\n\n        return service_pred, activity_pred, pooled_output\n\nclass CodeBertPredictor:\n    def __init__(self, model_path, training_data_path, model_name=\"microsoft/codebert-base\"):\n        # Device configuration\n        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        print(f\"Using device: {self.device}\")\n\n        # Predefined activities\n        self.predefined_activities = [\n            \"Login\", \"Logout\", \"Access\", \"Create\", \"Update\", \"Delete\",\n            \"View\", \"Edit\", \"Share\", \"Download\", \"Upload\",\n            \"Request\", \"Timeout\", \"Error\"\n        ]\n\n        # Load training data for label encoding\n        self.training_df = pd.read_csv(training_data_path, low_memory=False)\n\n        # Prepare data and tokenizer\n        self._prepare_data(model_name)\n\n        # Load trained model\n        self._load_model(model_path)\n\n    def _prepare_data(self, model_name):\n        # Validate and clean data\n        self.training_df['service'] = self.training_df['service'].fillna('Unknown')\n        self.training_df['activityType'] = self.training_df['activityType'].fillna('Unknown')\n\n        # Tokenizer\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n\n        # Label encoding\n        self.service_encoder = LabelEncoder()\n        self.activity_encoder = LabelEncoder()\n\n        # Fit encoders on training data\n        self.service_encoder.fit(self.training_df['service'])\n        self.activity_encoder.fit(self.training_df['activityType'])\n\n    def _load_model(self, model_path):\n        # Model initialization\n        service_num_labels = len(self.service_encoder.classes_)\n        activity_num_labels = len(self.activity_encoder.classes_)\n\n        # Create model\n        self.model = CodeBertTransformer(service_num_labels, activity_num_labels).to(self.device)\n\n        # Load trained weights\n        self.model.load_state_dict(torch.load(model_path, map_location=self.device))\n        self.model.eval()\n\n    def _prepare_text_features(self, row):\n        # Enhanced feature extraction with technical context\n        technical_features = [\n            str(row.get('url', '')),\n            str(row.get('method', '')),\n            str(row.get('headers_Host', '')),\n            str(row.get('requestHeaders_Content_Type', '')),\n            str(row.get('responseHeaders_Content_Type', ''))\n        ]\n\n        # Join features, limit length\n        return \" \".join(technical_features)[:512]\n\n    def predict(self, test_df):\n        # Prepare text features\n        test_texts = test_df.apply(self._prepare_text_features, axis=1)\n\n        # Tokenize test data\n        encodings = self.tokenizer(\n            test_texts.tolist(),\n            truncation=True,\n            padding=True,\n            max_length=128,\n            return_tensors='pt'\n        )\n\n        # Disable gradient computation\n        with torch.no_grad():\n            # Move data to device\n            input_ids = encodings['input_ids'].to(self.device)\n            attention_mask = encodings['attention_mask'].to(self.device)\n\n            # Forward pass\n            service_pred, activity_pred, embeddings = self.model(input_ids, attention_mask)\n\n            # Apply softmax to get probabilities\n            service_probs = F.softmax(service_pred, dim=1)\n            activity_probs = F.softmax(activity_pred, dim=1)\n\n            # Get top predictions and confidences\n            service_max_probs, service_preds = torch.max(service_probs, dim=1)\n            activity_max_probs, activity_preds = torch.max(activity_probs, dim=1)\n\n            # Convert to numpy for easier processing\n            service_preds = service_preds.cpu().numpy()\n            service_max_probs = service_max_probs.cpu().numpy()\n            activity_preds = activity_preds.cpu().numpy()\n            activity_max_probs = activity_max_probs.cpu().numpy()\n\n            # Calculate overall confidence scores\n            overall_service_confidence = float(np.mean(service_max_probs))\n            overall_activity_confidence = float(np.mean(activity_max_probs))\n\n            # Print only overall confidence scores\n            print(\"\\nOverall Confidence Scores:\")\n            print(f\"Service Confidence: {overall_service_confidence:.4f}\")\n            print(f\"Activity Confidence: {overall_activity_confidence:.4f}\")\n\n        return test_df\n\nclass CodeBertPredictorWithZSL(CodeBertPredictor):\n    def __init__(self, model_path, training_data_path, zsl_model, model_name=\"microsoft/codebert-base\"):\n        super().__init__(model_path, training_data_path, model_name)\n        # Initialize Zero-Shot Learner\n        self.zsl_model = zsl_model\n\ndef main():\n    # Paths\n    training_data_path = '/kaggle/input/network-dataset/shuffled_train.csv'\n    test_data_path = '/kaggle/input/network-dataset/shuffled_test.csv'\n    model_path = 'best_codebert_model.pth'\n\n    try:\n        # Initialize Zero-Shot Activity Predictor\n        predefined_activities = [\n            \"Login\", \"Logout\", \"Access\", \"Create\", \"Update\", \"Delete\",\n            \"View\", \"Edit\", \"Share\", \"Download\", \"Upload\",\n            \"Request\", \"Timeout\", \"Error\"\n        ]\n        zsl_model = ZeroShotActivityPredictor(predefined_activities)\n\n        # Load test data\n        test_df = pd.read_csv(test_data_path, low_memory=False)\n\n        # Initialize predictor with ZSL\n        predictor = CodeBertPredictorWithZSL(\n            model_path=model_path,\n            training_data_path=training_data_path,\n            zsl_model=zsl_model\n        )\n\n        # Predict with confidence threshold\n        predictions_df = predictor.predict(test_df)\n\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        import traceback\n        traceback.print_exc()\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T10:30:06.513402Z","iopub.execute_input":"2024-12-26T10:30:06.513657Z","iopub.status.idle":"2024-12-26T10:30:12.743687Z","shell.execute_reply.started":"2024-12-26T10:30:06.513635Z","shell.execute_reply":"2024-12-26T10:30:12.742785Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n<ipython-input-32-e78207b03060>:143: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  self.model.load_state_dict(torch.load(model_path, map_location=self.device))\n","output_type":"stream"},{"name":"stdout","text":"\nOverall Confidence Scores:\nService Confidence: 0.9790\nActivity Confidence: 0.9941\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport pandas as pd\nimport numpy as np\nfrom transformers import AutoTokenizer, AutoModel\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom collections import defaultdict\n\nclass ZeroShotActivityPredictor:\n    def __init__(self, activity_labels, model_name=\"microsoft/codebert-base\"):\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        self.model = AutoModel.from_pretrained(model_name)\n        self.activity_labels = activity_labels\n        self.activity_embeddings = self._get_activity_embeddings()\n\n    def _get_activity_embeddings(self):\n        activity_embeddings = {}\n        for activity in self.activity_labels:\n            inputs = self.tokenizer(activity, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n            with torch.no_grad():\n                outputs = self.model(**inputs)\n            activity_embeddings[activity] = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n        return activity_embeddings\n\n    def predict(self, activity_text):\n        inputs = self.tokenizer(activity_text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n        with torch.no_grad():\n            outputs = self.model(**inputs)\n        activity_embedding = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n\n        similarities = {}\n        for activity, embedding in self.activity_embeddings.items():\n            sim = cosine_similarity(activity_embedding, embedding)[0, 0]  # Extract scalar value\n            similarities[activity] = sim\n\n        best_match = max(similarities, key=similarities.get)\n        return best_match, similarities[best_match]\n\nclass CodeBertTransformer(nn.Module):\n    def __init__(self, service_num_labels, activity_num_labels, model_name=\"microsoft/codebert-base\"):\n        super().__init__()\n        self.transformer = AutoModel.from_pretrained(model_name)\n        self.dropout = nn.Dropout(0.3)\n        self.service_classifier = nn.Sequential(\n            nn.Linear(self.transformer.config.hidden_size, 512),\n            nn.BatchNorm1d(512),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(512, service_num_labels)\n        )\n        self.activity_classifier = nn.Sequential(\n            nn.Linear(self.transformer.config.hidden_size, 512),\n            nn.BatchNorm1d(512),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(512, activity_num_labels)\n        )\n\n    def forward(self, input_ids, attention_mask):\n        outputs = self.transformer(input_ids=input_ids, attention_mask=attention_mask)\n        pooled_output = outputs.last_hidden_state[:, 0, :]\n        pooled_output = self.dropout(pooled_output)\n        service_pred = self.service_classifier(pooled_output)\n        activity_pred = self.activity_classifier(pooled_output)\n        return service_pred, activity_pred, pooled_output\n\nclass CodeBertPredictor:\n    def __init__(self, model_path, training_data_path, model_name=\"microsoft/codebert-base\"):\n        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        print(f\"Using device: {self.device}\")\n        \n        self.predefined_activities = [\n            \"Login\", \"Logout\", \"Access\", \"Create\", \"Update\", \"Delete\",\n            \"View\", \"Edit\", \"Share\", \"Download\", \"Upload\",\n            \"Request\", \"Timeout\", \"Error\"\n        ]\n        \n        self.training_df = pd.read_csv(training_data_path, low_memory=False)\n        self._prepare_data(model_name)\n        self._load_model(model_path)\n\n    def _prepare_data(self, model_name):\n        self.training_df['service'] = self.training_df['service'].fillna('Unknown')\n        self.training_df['activityType'] = self.training_df['activityType'].fillna('Unknown')\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        self.service_encoder = LabelEncoder()\n        self.activity_encoder = LabelEncoder()\n        self.service_encoder.fit(self.training_df['service'])\n        self.activity_encoder.fit(self.training_df['activityType'])\n\n    def _load_model(self, model_path):\n        service_num_labels = len(self.service_encoder.classes_)\n        activity_num_labels = len(self.activity_encoder.classes_)\n        self.model = CodeBertTransformer(service_num_labels, activity_num_labels).to(self.device)\n        self.model.load_state_dict(torch.load(model_path, map_location=self.device))\n        self.model.eval()\n\n    def _prepare_text_features(self, row):\n        technical_features = [\n            str(row.get('url', '')),\n            str(row.get('method', '')),\n            str(row.get('headers_Host', '')),\n            str(row.get('requestHeaders_Content_Type', '')),\n            str(row.get('responseHeaders_Content_Type', ''))\n        ]\n        return \" \".join(technical_features)[:512]\n\n    def predict(self, test_df):\n        test_texts = test_df.apply(self._prepare_text_features, axis=1)\n        encodings = self.tokenizer(\n            test_texts.tolist(),\n            truncation=True,\n            padding=True,\n            max_length=128,\n            return_tensors='pt'\n        )\n\n        service_confidences = defaultdict(list)\n        activity_confidences = defaultdict(list)\n        all_predictions = {\n            'predicted_service': [],\n            'service_confidence': [],\n            'predicted_activity': [],\n            'activity_confidence': []\n        }\n\n        with torch.no_grad():\n            input_ids = encodings['input_ids'].to(self.device)\n            attention_mask = encodings['attention_mask'].to(self.device)\n            service_pred, activity_pred, embeddings = self.model(input_ids, attention_mask)\n            \n            service_probs = F.softmax(service_pred, dim=1)\n            activity_probs = F.softmax(activity_pred, dim=1)\n            \n            service_max_probs, service_preds = torch.max(service_probs, dim=1)\n            activity_max_probs, activity_preds = torch.max(activity_probs, dim=1)\n            \n            service_max_probs = service_max_probs.cpu().numpy()\n            service_preds = service_preds.cpu().numpy()\n            activity_max_probs = activity_max_probs.cpu().numpy()\n            activity_preds = activity_preds.cpu().numpy()\n\n            for i in range(len(service_preds)):\n                # Handle service predictions\n                service = self.service_encoder.inverse_transform([service_preds[i]])[0]\n                service_conf = float(service_max_probs[i])\n                service_confidences[service].append(service_conf)\n                all_predictions['predicted_service'].append(service)\n                all_predictions['service_confidence'].append(service_conf)\n\n                # Handle activity predictions\n                activity = self.activity_encoder.inverse_transform([activity_preds[i]])[0]\n                if activity.lower() == 'unknown':\n                    mapped_activity, confidence = self.zsl_model.predict(test_texts[i])\n                    activity_conf = float(confidence.item())  # Extract scalar value from numpy array\n                else:\n                    mapped_activity = activity\n                    activity_conf = float(activity_max_probs[i])\n                \n                activity_confidences[mapped_activity].append(activity_conf)\n                all_predictions['predicted_activity'].append(mapped_activity)\n                all_predictions['activity_confidence'].append(activity_conf)\n\n            # Calculate overall scores\n            overall_service_confidence = np.mean(service_max_probs)\n            overall_activity_confidence = np.mean([conf for conf_list in activity_confidences.values() for conf in conf_list])\n\n            # Print results\n            print(\"\\n=== Overall Confidence Scores ===\")\n            print(f\"Service Confidence: {float(overall_service_confidence):.4f}\")\n            print(f\"Activity Confidence: {float(overall_activity_confidence):.4f}\")\n\n            print(\"\\n=== Service Confidence Scores ===\")\n            for service, confidences in sorted(service_confidences.items()):\n                mean_conf = np.mean(confidences)\n                count = len(confidences)\n                print(f\"{service:30} Confidence: {float(mean_conf):.4f} (Count: {count})\")\n\n            print(\"\\n=== Activity Confidence Scores ===\")\n            for activity, confidences in sorted(activity_confidences.items()):\n                mean_conf = np.mean(confidences)\n                count = len(confidences)\n                print(f\"{activity:30} Confidence: {float(mean_conf):.4f} (Count: {count})\")\n\n        # Add predictions to DataFrame\n        for key, values in all_predictions.items():\n            test_df[key] = values\n\n        return test_df\n\nclass CodeBertPredictorWithZSL(CodeBertPredictor):\n    def __init__(self, model_path, training_data_path, zsl_model, model_name=\"microsoft/codebert-base\"):\n        super().__init__(model_path, training_data_path, model_name)\n        self.zsl_model = zsl_model\n\ndef main():\n    training_data_path = '/kaggle/input/network-dataset/shuffled_train.csv'\n    test_data_path = '/kaggle/input/network-dataset/shuffled_test.csv'\n    model_path = 'best_codebert_model.pth'\n\n    try:\n        predefined_activities = [\n            \"Login\", \"Logout\", \"Access\", \"Create\", \"Update\", \"Delete\",\n            \"View\", \"Edit\", \"Share\", \"Download\", \"Upload\",\n            \"Request\", \"Timeout\", \"Error\"\n        ]\n        zsl_model = ZeroShotActivityPredictor(predefined_activities)\n        test_df = pd.read_csv(test_data_path, low_memory=False)\n        predictor = CodeBertPredictorWithZSL(\n            model_path=model_path,\n            training_data_path=training_data_path,\n            zsl_model=zsl_model\n        )\n        predictions_df = predictor.predict(test_df)\n\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        import traceback\n        traceback.print_exc()\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T10:30:19.495503Z","iopub.execute_input":"2024-12-26T10:30:19.495811Z","iopub.status.idle":"2024-12-26T10:30:46.628294Z","shell.execute_reply.started":"2024-12-26T10:30:19.495789Z","shell.execute_reply":"2024-12-26T10:30:46.627371Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n<ipython-input-33-933ec69e0d96>:98: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  self.model.load_state_dict(torch.load(model_path, map_location=self.device))\n","output_type":"stream"},{"name":"stdout","text":"\n=== Overall Confidence Scores ===\nService Confidence: 0.9790\nActivity Confidence: 0.9898\n\n=== Service Confidence Scores ===\n4shared                        Confidence: 0.9895 (Count: 126)\nBox                            Confidence: 0.9935 (Count: 95)\nDropbox                        Confidence: 0.9918 (Count: 119)\nIcedrive                       Confidence: 0.9478 (Count: 139)\nJumpshare                      Confidence: 0.9874 (Count: 25)\nKoofr                          Confidence: 0.9909 (Count: 48)\nMediaFire                      Confidence: 0.9919 (Count: 68)\nOneDrive                       Confidence: 0.9665 (Count: 143)\nZippyshare                     Confidence: 0.9853 (Count: 21)\npCloud                         Confidence: 0.9838 (Count: 16)\n\n=== Activity Confidence Scores ===\nDownload                       Confidence: 0.9962 (Count: 203)\nEdit                           Confidence: 0.9761 (Count: 4)\nError                          Confidence: 0.9540 (Count: 26)\nLogin                          Confidence: 0.9972 (Count: 192)\nLogout                         Confidence: 0.9722 (Count: 21)\nShare                          Confidence: 0.9726 (Count: 148)\nUpload                         Confidence: 0.9957 (Count: 206)\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}